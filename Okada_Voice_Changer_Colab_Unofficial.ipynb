{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/freyzamarshall02/w-okadavoicechangercolab/blob/main/Okada_Voice_Changer_Colab_Unofficial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EnR6lSmXBCN"
      },
      "source": [
        "<h1 style=\"text-align: center;\">\n",
        "  <span style=\"color: #00ffff;\">OKADA VOICE CHANGER COLAB (UNOFFICIAL)</span>\n",
        "</h1>\n",
        "\n",
        "<hr />\n",
        "  <h2 style=\"text-align: center;\">\n",
        "\n",
        "  <h2 style=\"text-align: center;\">\n",
        "    <span style=\"color: #FFFFFF;\">AI Realtime Voice Changer On Google Colab</span>\n",
        "\n",
        "   <h2 style=\"text-align: center;\">\n",
        "    <span style=\"color: #FFFFFF;\">Notebook By FreyzaMarshall</span>\n",
        "\n",
        "  </h2>\n",
        "  </span>\n",
        "  </h2>\n",
        "  <a href=\"https://ko-fi.com/freyzamarshall\" target=\"_parent\"><img src=\"https://img.shields.io/badge/Ko--fi-F16061?style=for-the-badge&logo=ko-fi&logoColor=white\" align=\"right\" alt=\"Open\"></a>\n",
        "\n",
        "   <a href=\"https://discord.gg/sr5kyhRy3x\" target=\"_parent\"><img src=\"https://img.shields.io/badge/Discord-5865F2?style=for-the-badge&logo=discord&logoColor=white\" align=\"right\" alt=\"Open\"></a>\n",
        "\n",
        "  <a href=\"https://www.youtube.com/@FreyzaMarshall_\" target=\"_parent\"><img src=\"https://img.shields.io/badge/YouTube-FF0000?style=for-the-badge&logo=youtube&logoColor=white\" align=\"right\" alt=\"Open\"></a>\n",
        "\n",
        "\n",
        "For mobile user may experience bugs and feature limitations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "FhcKp_4FXFRR"
      },
      "outputs": [],
      "source": [
        "#=================Updated=================\n",
        "# @title **Cell[1]** Clone repository and install dependencies\n",
        "# @markdown This first step will download the latest version of Voice Changer and install the dependencies. **It can take some time to complete.**\n",
        "import os\n",
        "import time\n",
        "import subprocess\n",
        "import threading\n",
        "import shutil\n",
        "import base64\n",
        "import codecs\n",
        "import torch\n",
        "import sys\n",
        "import requests\n",
        "from IPython.display import clear_output\n",
        "from typing import Literal, TypeAlias\n",
        "from google.colab import drive\n",
        "\n",
        "# Fix some packages\n",
        "!pip install pip==24.0.0 -q\n",
        "\n",
        "Mode: TypeAlias = Literal[\"elf\", \"zip\"]\n",
        "mode:Mode=\"elf\"\n",
        "\n",
        "# Configs\n",
        "%cd /content\n",
        "Run_Cell=0\n",
        "\n",
        "#@markdown Pilih Versi Okada Voice Changer / Choose Okada Voice Changer Version\n",
        "version = \"V1(old)\" #@param [\"V1(old)\", \"V1(new)\", \"V2\"]\n",
        "\n",
        "current_version_hash=None\n",
        "latest_version_hash=None\n",
        "\n",
        "# Check GPU\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU is available\")\n",
        "    print(\"GPU Name:\", torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print(\"GPU is not available\")\n",
        "    # sys.exit(\"No GPU available. Change runtime.\")\n",
        "\n",
        "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
        "\n",
        "if version == \"V1(new)\":\n",
        "    print('\\033[36m\\033[1m\\033[3mDownloading prebuilt executable...\\033[0m')\n",
        "    res = requests.get('https://api.github.com/repos/deiteris/voice-changer/releases/latest')\n",
        "    release_info = res.json()\n",
        "\n",
        "    for asset in release_info['assets']:\n",
        "        if not asset['name'].startswith('voice-changer-linux-amd64-cuda.tar.gz'):\n",
        "            continue\n",
        "        download_url = asset['browser_download_url']\n",
        "        !wget -q --show-progress {download_url}\n",
        "\n",
        "    print('\\033[32m\\033[1m\\033[3mUnpacking...\\033[0m')\n",
        "    !cat voice-changer-linux-amd64-cuda.tar.gz.* | tar xzf -\n",
        "    !rm -rf voice-changer-linux-amd64-cuda.tar.gz.*\n",
        "    print('\\033[32m\\033[1m\\033[3mFinished unpacking!\\033[0m')\n",
        "\n",
        "    path = codecs.decode('ZZIPFreireFVB', 'rot_13')\n",
        "\n",
        "    %cd $path\n",
        "\n",
        "    # Additional\n",
        "    !pip install python-dotenv pyngrok --quiet\n",
        "\n",
        "    print('\\033[32m\\033[1m\\033[3mSuccessfully downloaded and unpacked the binary!!\\033[0m')\n",
        "    # libportaudio2\n",
        "    print('\\033[36m\\033[1m\\033[3mInstalling libportaudio2...\\033[0m')\n",
        "    !apt-get -y install libportaudio2 -qq\n",
        "\n",
        "    # Server Config\n",
        "    %cd /content/$path\n",
        "\n",
        "    from dotenv import set_key\n",
        "\n",
        "    set_key('.env', \"SAMPLE_MODE\", \"\")\n",
        "\n",
        "    Ready = True\n",
        "\n",
        "    clear_output()\n",
        "    print('\\033[32m\\033[1m\\033[3mCell 1 Was Executed Completely\\033[0m')\n",
        "\n",
        "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
        "\n",
        "elif version == \"V1(old)\":\n",
        "\n",
        "    from IPython.display import clear_output\n",
        "\n",
        "    # Define the URL and the destination paths\n",
        "    url = 'https://huggingface.co/freyza/w-okada/resolve/main/w-okada.zip'\n",
        "    zip_path = '/content/w-okada.zip'\n",
        "    extract_path = '/content'\n",
        "\n",
        "    # Download the zip file\n",
        "    subprocess.run(['wget', '-q', '-O', zip_path, url], check=True)\n",
        "\n",
        "    # Unzip the downloaded file to the specified directory\n",
        "    subprocess.run(['unzip', '-o', '-q', zip_path, '-d', extract_path], check=True)\n",
        "\n",
        "    # Remove the zip file after extraction\n",
        "    subprocess.run(['rm', zip_path], check=True)\n",
        "\n",
        "    # List the contents to verify\n",
        "    subprocess.run(['ls', extract_path], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "\n",
        "    # Install libportaudio2\n",
        "    %cd /content/w-okada/server\n",
        "    print('\\033[36m\\033[1m\\033[3mInstalling libportaudio2...\\033[0m')\n",
        "    !apt-get -y install libportaudio2 -qq\n",
        "\n",
        "    # Install additional dependencies\n",
        "    print('\\033[36m\\033[1m\\033[3mInstalling pre-dependencies...\\033[0m')\n",
        "    !python -m pip install ort-nightly-gpu --index-url=https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/ort-cuda-12-nightly/pypi/simple/ -q\n",
        "    !pip install faiss-gpu fairseq pyngrok --quiet\n",
        "    !pip install pyworld --no-build-isolation --quiet\n",
        "\n",
        "    # Install dependencies from requirements.txt\n",
        "    print('\\033[36m\\033[1m\\033[3mInstalling dependencies from requirements.txt...\\033[0m')\n",
        "    !pip install -r requirements.txt --quiet\n",
        "\n",
        "    clear_output()\n",
        "    print('\\033[32m\\033[1m\\033[3mCell 1 Was Executed Completely\\033[0m')\n",
        "\n",
        "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
        "\n",
        "elif version == \"V2\":\n",
        "\n",
        "    from IPython.display import clear_output, Javascript\n",
        "\n",
        "    work_dir = \"/content\"\n",
        "    print(\"Downloading the latest vcclient... \")\n",
        "    !curl -s -L https://huggingface.co/wok000/vcclient000_colab/resolve/main/latest_hash.txt -o latest_hash.txt\n",
        "    latest_version_hash = open('latest_hash.txt').read().strip()\n",
        "\n",
        "    if mode == \"elf\":\n",
        "        !curl -L https://huggingface.co/wok000/vcclient000_colab/resolve/main/vcclient_latest_for_colab -o {work_dir}/vcclient_latest_for_colab\n",
        "    elif mode == \"zip\":\n",
        "        !curl -L https://huggingface.co/wok000/vcclient000_colab/resolve/main/vcclient_latest_for_colab -o {work_dir}/vcclient_latest_for_colab.zip\n",
        "\n",
        "    print(\"Download is done.\")\n",
        "\n",
        "    if current_version_hash != latest_version_hash and mode == \"zip\":\n",
        "        print(f\"Unzip vcclient to {latest_version_hash} ... \")\n",
        "        !cd {work_dir} && unzip -q vcclient_latest_for_colab.zip -d {latest_version_hash}\n",
        "        print(f\"Unzip is done.\")\n",
        "\n",
        "    if mode == \"elf\":\n",
        "        %cd {work_dir}\n",
        "        !chmod 0700 vcclient_latest_for_colab\n",
        "    elif mode == \"zip\":\n",
        "        %cd {work_dir}/{latest_version_hash}/main\n",
        "        !chmod 0700 main\n",
        "\n",
        "    print(\"Installing modules... \", end=\"\")\n",
        "    !sudo apt-get install -y libportaudio2 > /dev/null 2>&1\n",
        "    !pip install pyngrok > /dev/null 2>&1\n",
        "    clear_output()\n",
        "    print('\\033[32m\\033[1m\\033[3mV2 Cell 1 Was Executed Completely\\033[0m')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8-mkn17pT2W",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "import codecs\n",
        "#=======================Updated=========================\n",
        "\n",
        "# @title **Cell[2]** Start Server **using ngrok**\n",
        "# @markdown This cell will start the server, the first time that you run it will download the models, so it can take a while (~1-2 minutes)\n",
        "\n",
        "# @markdown ---\n",
        "# @markdown You'll need a ngrok account, but <font color=green>**it's free**</font> and easy to create!\n",
        "# @markdown ---\n",
        "# @markdown **1** - Create a <font color=green>**free**</font> account at [ngrok](https://dashboard.ngrok.com/signup) or **login with Google/Github account**\\\n",
        "# @markdown **2** - If you didn't logged in with Google/Github, you will need to **verify your e-mail**!\\\n",
        "# @markdown **3** - Click [this link](https://dashboard.ngrok.com/get-started/your-authtoken) to get your auth token, and place it here:\n",
        "Token = '' # @param {type:\"string\"}\n",
        "# @markdown **4** - *(optional)* Change to a region near to you or keep at United States if increase latency\\\n",
        "# @markdown `Default Region: ap - Asia/Pacific (Singapore)`\n",
        "Region = \"ap - Asia/Pacific (Singapore)\" # @param [\"ap - Asia/Pacific (Singapore)\", \"au - Australia (Sydney)\",\"eu - Europe (Frankfurt)\", \"in - India (Mumbai)\",\"jp - Japan (Tokyo)\",\"sa - South America (Sao Paulo)\", \"us - United States (Ohio)\"]\n",
        "\n",
        "#@markdown **5** - *(optional)* Other options:\n",
        "ClearConsole = True  # @param {type:\"boolean\"}\n",
        "\n",
        "# ---------------------------------\n",
        "# DO NOT TOUCH ANYTHING DOWN BELOW!\n",
        "# ---------------------------------\n",
        "if version == \"V1(new)\":\n",
        "    %cd /content/MMVCServerSIO\n",
        "    if not globals().get('Ready', False):\n",
        "        print(\"Go back and run first cells.\")\n",
        "    else:\n",
        "        from pyngrok import conf, ngrok\n",
        "        MyConfig = conf.PyngrokConfig()\n",
        "        MyConfig.auth_token = Token\n",
        "        MyConfig.region = Region[0:2]\n",
        "        conf.get_default().authtoken = Token\n",
        "        conf.get_default().region = Region\n",
        "        conf.set_default(MyConfig)\n",
        "\n",
        "        import threading, time, socket\n",
        "        PORT = 18888\n",
        "\n",
        "        import json\n",
        "        from pyngrok import ngrok\n",
        "        from IPython.display import clear_output\n",
        "\n",
        "        ngrokConnection = ngrok.connect(PORT)\n",
        "        public_url = ngrokConnection.public_url\n",
        "        set_key('.env', \"ALLOWED_ORIGINS\", json.dumps([public_url]))\n",
        "\n",
        "        def wait_for_server():\n",
        "            while True:\n",
        "                time.sleep(0.5)\n",
        "                sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
        "                result = sock.connect_ex(('127.0.0.1', PORT))\n",
        "                if result == 0:\n",
        "                    break\n",
        "                sock.close()\n",
        "            if ClearConsole:\n",
        "                clear_output()\n",
        "            print(\"--------- SERVER READY! ---------\")\n",
        "            print(\"Your server is available at:\")\n",
        "            print(public_url)\n",
        "            print(\"---------------------------------\")\n",
        "\n",
        "        threading.Thread(target=wait_for_server, daemon=True).start()\n",
        "\n",
        "        !./$path\n",
        "\n",
        "        ngrok.disconnect(ngrokConnection.public_url)\n",
        "        print('\\033[32m\\033[1m\\033[3mServer Telah Berhenti / Server Was Stopped\\033[0m')\n",
        "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
        "\n",
        "elif version == \"V1(old)\":\n",
        "\n",
        "    from pyngrok import conf, ngrok\n",
        "    MyConfig = conf.PyngrokConfig()\n",
        "    MyConfig.auth_token = Token\n",
        "    MyConfig.region = Region[0:2]\n",
        "    #conf.get_default().authtoken = Token\n",
        "    #conf.get_default().region = Region\n",
        "    conf.set_default(MyConfig)\n",
        "\n",
        "    import subprocess, threading, time, socket, urllib.request\n",
        "    PORT = 9999\n",
        "\n",
        "    from pyngrok import ngrok\n",
        "    ngrokConnection = ngrok.connect(PORT)\n",
        "    public_url = ngrokConnection.public_url\n",
        "\n",
        "    from IPython.display import clear_output\n",
        "\n",
        "    def wait_for_server():\n",
        "        while True:\n",
        "            time.sleep(0.5)\n",
        "            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
        "            result = sock.connect_ex(('127.0.0.1', PORT))\n",
        "            if result == 0:\n",
        "                break\n",
        "            sock.close()\n",
        "        if ClearConsole:\n",
        "            clear_output()\n",
        "        print(\"--------- SERVER READY! ---------\")\n",
        "        print(\"Your server is available at:\")\n",
        "        print(public_url)\n",
        "        print(\"---------------------------------\")\n",
        "\n",
        "    threading.Thread(target=wait_for_server, daemon=True).start()\n",
        "\n",
        "    !python3 pengubahsuara.py \\\n",
        "      -p {PORT} \\\n",
        "      --https False \\\n",
        "      --content_vec_500 pretrain/checkpoint_best_legacy_500.pt \\\n",
        "      --content_vec_500_onnx pretrain/content_vec_500.onnx \\\n",
        "      --content_vec_500_onnx_on false \\\n",
        "      --hubert_base pretrain/hubert_base.pt \\\n",
        "      --hubert_base_jp pretrain/rinna_hubert_base_jp.pt \\\n",
        "      --hubert_soft pretrain/hubert/hubert-soft-0d54a1f4.pt \\\n",
        "      --nsf_hifigan pretrain/nsf_hifigan/model \\\n",
        "      --crepe_onnx_full pretrain/crepe_onnx_full.onnx \\\n",
        "      --crepe_onnx_tiny pretrain/crepe_onnx_tiny.onnx \\\n",
        "      --rmvpe pretrain/rmvpe.pt \\\n",
        "      --model_dir model_dir \\\n",
        "      --samples samples.json\n",
        "\n",
        "    ngrok.disconnect(ngrokConnection.public_url)\n",
        "    print('\\033[32m\\033[1m\\033[3mServer Telah Berhenti / Server Was Stopped\\033[0m')\n",
        "\n",
        "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
        "\n",
        "elif version == \"V2\":\n",
        "\n",
        "    import time\n",
        "    from pyngrok import ngrok\n",
        "    from IPython.display import clear_output, display, Javascript, Audio\n",
        "\n",
        "    Close_Ngrok=True\n",
        "    Open_New_Tab=True\n",
        "    PORT = 8003\n",
        "    LOG_FILE = f\"/content/LOG_FILE_{PORT}\"\n",
        "\n",
        "    def play_notification_sound(url):\n",
        "        display(Audio(url=url, autoplay=True))\n",
        "    Close_Ngrok = True\n",
        "    Open_New_Tab = True\n",
        "\n",
        "    # Start\n",
        "    if mode == \"elf\":\n",
        "        get_ipython().system_raw(f'LD_LIBRARY_PATH=/usr/lib64-nvidia:/usr/lib/x86_64-linux-gnu ./vcclient_latest_for_colab cui --port {PORT} --no_cui true >{LOG_FILE} 2>&1 &')\n",
        "    elif mode == \"zip\":\n",
        "        !LD_LIBRARY_PATH=/usr/lib64-nvidia:/usr/lib/x86_64-linux-gnu ./main cui --port {PORT} --no_cui true &\n",
        "\n",
        "    if Close_Ngrok:\n",
        "        ngrok.kill()\n",
        "\n",
        "    ngrok.set_auth_token(Token)\n",
        "\n",
        "    # Tunggu sampai server dimulai\n",
        "    print('\\033[31m\\033[1m\\033[3mTunggu sampai server dimulai\\033[0m')\n",
        "    time.sleep(130)\n",
        "\n",
        "    main_tunnel = ngrok.connect(PORT)\n",
        "    clear_output()\n",
        "\n",
        "    if Open_New_Tab:\n",
        "        display(Javascript(f'window.open(\"{main_tunnel.public_url}\", \"_blank\");'))\n",
        "\n",
        "    print(\"--------- SERVER LINK ---------\")\n",
        "    print('\\033[32m\\033[1m\\033[3mServer Sudah Berjalan\\033[0m')\n",
        "    print(\"PUBLIC URL:\", main_tunnel.public_url)\n",
        "    print(\"---------------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import requests\n",
        "import codecs\n",
        "\n",
        "if version == \"V1(old)\":\n",
        "\n",
        "    #@title **[Optional Cell For V1 Old Okada Voice Changer]** Upload a voice model (Run this before running the Voice Changer)\n",
        "    #@markdown Find your model here [voice-models](https://voice-models.com/)\n",
        "    # @markdown ---\n",
        "    model_slot = \"0\" #@param ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199']\n",
        "\n",
        "    !rm -rf model_dir/$model_slot\n",
        "    #@markdown **[Optional]** Add an icon to the model (Kosongin aja gapapa / It's okay to leave it blank)\n",
        "    icon_link = \"\"  #@param {type:\"string\"}\n",
        "    icon_link = '\"'+icon_link+'\"'\n",
        "    !mkdir model_dir\n",
        "    !mkdir model_dir/$model_slot\n",
        "    #@markdown Put your model's download link here `(must be a zip file and don't use GPT-SoVITS Model)` only supports **huggingface.co** & **google drive**<br>\n",
        "    model_link = \"\"  #@param {type:\"string\"}\n",
        "\n",
        "    if model_link.startswith(\"https://www.weights.gg\") or model_link.startswith(\"https://weights.gg\"):\n",
        "        print(\"Links from weights.gg is no longer supported.\")\n",
        "        sys.exit()\n",
        "    elif model_link.startswith(\"https://drive.google.com\"):\n",
        "        model_link = '\"'+model_link+'\"'\n",
        "        !gdown $model_link --fuzzy -O model.zip\n",
        "        print(\"Model from Drive\")\n",
        "    elif model_link.startswith(\"https://huggingface.co\"):\n",
        "        model_link = model_link\n",
        "        model_link = '\"'+model_link+'\"'\n",
        "        !curl -L $model_link > model.zip\n",
        "        print(\"Model from huggingface Link\")\n",
        "    else:\n",
        "        model_link = model_link\n",
        "        model_link = '\"'+model_link+'\"'\n",
        "        !curl -L -O $model_link\n",
        "        !mv ./*.pth model_dir/$model_slot/\n",
        "        print('Model(.pth) or a direct model link.')\n",
        "\n",
        "    # Conditionally set the iconFile based on whether icon_link is empty\n",
        "    if icon_link == '\"\"':\n",
        "        iconFile = \"\"\n",
        "        print(\"icon_link is empty, so no icon file will be downloaded.\")\n",
        "    else:\n",
        "        iconFile = \"icon.png\"\n",
        "        !curl -L $icon_link > model_dir/$model_slot/icon.png\n",
        "\n",
        "    !unzip model.zip -d model_dir/$model_slot\n",
        "\n",
        "    !mv model_dir/$model_slot/*/* model_dir/$model_slot/\n",
        "    !rm -rf model_dir/$model_slot/*/\n",
        "    !rm -rf model.zip\n",
        "    #@markdown **Model Voice Conversion Setting**\n",
        "    Tune = 0  #@param {type:\"slider\",min:-24,max:24,step:1}\n",
        "    Index = 0  #@param {type:\"slider\",min:0,max:1,step:0.1}\n",
        "\n",
        "    param_link = \"\"\n",
        "    if param_link == \"\":\n",
        "        paramset = requests.get(\"https://pastebin.com/raw/P6ar4baa\").text\n",
        "        exec(paramset)\n",
        "\n",
        "    clear_output()\n",
        "    print(\"\\033[93mModel with the name of \"+model_name+\" has been Imported to slot \"+model_slot)\n",
        "\n",
        "elif version == \"V1(new)\":\n",
        "    print('\\033[31m\\033[1m\\033[3mV1 New Belum Support / V1 New is not supported\\033[0m')\n",
        "elif version == \"V2\":\n",
        "    print('\\033[31m\\033[1m\\033[3mV2 Belum Support / V2 is not supported\\033[0m')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "N2c7UpbA3eYr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}